{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50616eee-4881-4045-ba08-0ffd7c7ffd97",
   "metadata": {},
   "source": [
    "# Asynchronous Algorithms\n",
    "\n",
    "This exercise will introduce CUDA streams and the `AthAsynchronousAlgorithm` which allows us to run GPU code asynchronously (i.e. without blocking a CPU thread).\n",
    "We will do this with a semi-contrived task calculating jet pull vectors using the CUB reduction algorithms mentioned last week.\n",
    "\n",
    "The build of the code is done in the usual way.\n",
    "  - On Perlmutter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6747a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./scripts/run_on_perlmutter.sh cmake -DCMAKE_BUILD_TYPE=Debug -S . -B build\n",
    "!./scripts/run_on_perlmutter.sh cmake --build build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c203dfb",
   "metadata": {},
   "source": [
    "  - On SWAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392f3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./scripts/run_on_swan.sh cmake -DCMAKE_BUILD_TYPE=Debug -S . -B build\n",
    "!./scripts/run_on_swan.sh cmake --build build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66b7ac-a52f-4ce9-99f1-80c6d673f017",
   "metadata": {},
   "source": [
    "Once the code's build is up to date, let's start by looking at the\n",
    "[job config](CUDAExamples/python/03_Asynchronous.py), and noting the aspects\n",
    "unique to a job with asynchronous algorithms (between the lines of hashes).\n",
    "\n",
    "Then, we'll run it as-is.\n",
    "  - On Perlmutter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36a16b-8196-4e8a-b2f9-a9a8d76af60b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./scripts/run_on_perlmutter.sh ./build/CMakeFiles/atlas_build_run.sh athena.py --threads=2 --CA CUDAExamples/03_Asynchronous.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7d98e",
   "metadata": {},
   "source": [
    "  - On SWAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87667d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./scripts/run_on_swan.sh ./build/CMakeFiles/atlas_build_run.sh athena.py --threads=2 --CA CUDAExamples/03_Asynchronous.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eade0b-320b-4782-abee-a8f2bf234191",
   "metadata": {},
   "source": [
    "Next we'll take a quick look at the [header](CUDAExamples/src/03_Asynchronous/JetPullCUDAAlg.h) and [C++ source](CUDAExamples/src/03_Asynchronous/JetPullCUDAAlg.cxx) files of our algorithm, to see the special features we need to include:\n",
    "* Inherit from `AthAsynchronousAlgorithm`, not `AthReentrantAlgorithm`\n",
    "* Include some kind of `deviceExecute` function called from our `execute` function\n",
    "  * In practice, this could be in a tool somewhere\n",
    "\n",
    "Now let's move on to the meat of the code, the `deviceExecute` function and adjoining [CUDA code](CUDAExamples/src/03_Asynchronous/JetPullCUDAAlg.cu)\n",
    "* Note the `Gaudi::CUDA::Stream` instantiated at the start. This is what we use to run multiple \"streams\" of CUDA code simultaneously. This is also what we use to determine when the fiber scheduler should wake us because the GPU is done with it's work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f2954-dd84-4017-8ec7-afe521f780b6",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "As a first task, write the calls to copy the input data into the arrays in `d_jet` and `d_const`. For now, don't include the constituent count information.\n",
    "\n",
    "You will be using the `cudaMemcpy` function with the following signature:\n",
    "`cudaMemcpyAsync(dest, source, num_bytes, direction: cudaMemcpyHostToDevice, stream)`\n",
    "\n",
    "If you get stuck, the solution is [here](CUDAExamples/src/03_Asynchronous/solution/JetPullCUDAAlg.cu.soln1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a360a7-fd8b-4d4d-943d-417a5772b0f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./scripts/run_on_perlmutter.sh ./build/CMakeFiles/atlas_build_run.sh athena.py --threads=2 --CA CUDAExamples/03_Asynchronous.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ee9c5-f466-4af0-a6bb-06097fc7e258",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Your next task is to calculate the offsets into the array of constituents from the constituent count information.\n",
    "You can do this using CUB's [DeviceScan](https://nvidia.github.io/cccl/cub/api/structcub_1_1DeviceScan.html) functionality.\n",
    "\n",
    "First allocate the array to hold the information on the device and copy the input into it, as before. Use `cudaMemsetAlloc` to set the last element of the array to zero. It has the same signature as `cudaMemcpyAsync` but the second argument is a value, and it needs no direction. When you complete this task, you also need to uncomment the line that frees the `d_offsets` array you will be allocating.\n",
    "\n",
    "Then, use `cub::DeviceScan::ExclusiveSum` to convert the counts into offsets. The key here is that you run it first with the `temp_storage` pointer set to a nullptr to determine how much storage to allocate (it returns immediately). You can then allocate the storage, and run it again to actually do the work. When it's done, remember to await the stream with `ATH_CHECK(stream.await());`, then free the temporary storage.\n",
    "\n",
    "If you get stuck, the solution is [here](CUDAExamples/src/03_Asynchronous/solution/JetPullCUDAAlg.cu.soln2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec08be-ce78-4ffb-b0b5-63f0e65dc53a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./scripts/run_on_perlmutter.sh ./build/CMakeFiles/atlas_build_run.sh athena.py --threads=2 --CA CUDAExamples/03_Asynchronous.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f18f0-43b1-4e40-897f-ad20022d2966",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Finally, we get to the heart of this exercise. You will write a kernel using [BlockReduce](https://nvidia.github.io/cccl/cub/api/classcub_1_1BlockReduce.html) to calculate the pulls, and also write the code to setup the output buffers, launch the kernel to run the calculation, then copy the output out to host memory, and finally free the output buffers.\n",
    "\n",
    "If you get stuck, feel free to ask questions. If you get really stuck, the solution is [here](CUDAExamples/src/03_Asynchronous/solution/JetPullCUDAAlg.cu.soln3).\n",
    "\n",
    "Note the kernel launch syntax:\n",
    "```cuda\n",
    "Kernels::calculatePulls<<<nJets, BLOCKSIZE, dynamic shared memory per stream: 0, stream>>>(...);\n",
    "```\n",
    "\n",
    "In your kernel you will use shared memory for the BlockReduce, but this is static, not dynamic. Also remember to call `__syncthreads()` before and after your block reductions.\n",
    "\n",
    "### Jet Pull Definition\n",
    "\n",
    "For the purpose of this task, the jet pull vector is defined using the following sum over constituents $c$ of jet $j$. The $\\vec{r}$ vectors are in $[\\eta, \\phi]$ space.\n",
    "\n",
    "$$\\vec{p}_{j} = \\sum_{c \\in j} \\left[ \\frac{p^{c}_{T}}{p^{j}_{T}} \\left|\\vec{r}_{c} - \\vec{r}_{j}\\right| \\left( \\vec{r}_{c} - \\vec{r}_{j} \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fba355-d405-4d88-8e58-65460ab4e11d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./scripts/run_on_perlmutter.sh ./build/CMakeFiles/atlas_build_run.sh athena.py --threads=2 --CA CUDAExamples/03_Asynchronous.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8c538-e9b5-44c9-aaab-cb28259e11b3",
   "metadata": {},
   "source": [
    "## Extensions for later\n",
    "* In the example, we use `std::span`s for the arrays copied directly out of the event store. Would it be worth copying these into pinned memory first?\n",
    "* How would you extend this to the correct definition of jet pull, using rapidities instead of pseudo-rapidities? Can you calculate the rapidities on the GPU?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
