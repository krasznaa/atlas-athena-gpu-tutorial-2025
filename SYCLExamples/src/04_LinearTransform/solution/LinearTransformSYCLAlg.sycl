// Copyright (C) 2002-2025 CERN for the benefit of the ATLAS collaboration

// Local include(s).
#include "LinearTransformSYCLAlg.h"

// SYCL include(s).
#include <sycl/sycl.hpp>

namespace GPUTutorial
{
   namespace Kernels
   {
      /// FIX Simple kernel for a parallel linear transformation.
      class LinearTransform
      {

      public:
         /// Constructor with the kernel parameters.
         LinearTransform(std::size_t size, float *input, float *output)
             : m_size{size}, m_input{input}, m_output{output}
         {
         }

         /// The operator that will be called by SYCL.
         void operator()(sycl::nd_item<1> item) const
         {
            // Check that we're in bounds.
            const std::size_t i = item.get_global_id(0);
            if (i >= m_size)
            {
               return;
            }

            // Perform a very simple linear transformation.
            m_output[i] = 2.0f * m_input[i] + 1.0f;
         }

      private:
         /// The size of the input and output arrays.
         std::size_t m_size;
         /// The input array.
         float *m_input;
         /// The output array.
         float *m_output;

      }; // class LinearTransform
      // FIX

   } // namespace Kernels

   StatusCode LinearTransformSYCLAlg::initialize()
   {
      // Simply greet the user.
      ATH_MSG_INFO("Initializing " << name() << "...");

      // Return gracefully.
      return StatusCode::SUCCESS;
   }

   StatusCode LinearTransformSYCLAlg::execute(const EventContext &ctx) const
   {
      // Set up a SYCL queue.
      sycl::queue queue;
      ATH_MSG_INFO("Using device: "
                   << queue.get_device().get_info<sycl::info::device::name>());

      // Set up an input array on the host.
      constexpr std::size_t n = 1000000;
      std::vector<float> inputHost(n);
      for (std::size_t i = 0; i < n; ++i)
      {
         inputHost[i] = static_cast<float>(i);
      }

      // Allocate input and output buffers on the device.
      float *inputDevice = sycl::malloc_device<float>(n, queue);
      float *outputDevice = sycl::malloc_device<float>(n, queue);

      // Copy the input data to the device.
      queue.memcpy(inputDevice, inputHost.data(), n * sizeof(float))
          .wait_and_throw();

      // FIX Carefully set up the ND range for the kernel.
      const std::size_t localRange = 256;
      const std::size_t globalRange = (n + localRange - 1) /
                                      localRange * localRange;
      const sycl::nd_range<1> ndRange{globalRange, localRange};
      // FIX

      // Run the kernel.
      queue.submit([&](sycl::handler &h)
                   {
                     Kernels::LinearTransform kernel(n, inputDevice, // FIX
                        outputDevice);                               // FIX
                     h.parallel_for<Kernels::LinearTransform>( // FIX
                         ndRange, kernel); }) // FIX
          .wait_and_throw();

      // Copy the output data back to the host.
      std::vector<float> outputHost(n);
      queue.memcpy(outputHost.data(), outputDevice, n * sizeof(float))
          .wait_and_throw();

      // Print some elements of the output.
      ATH_MSG_INFO("outputHost[0]      = " << outputHost[0]);
      ATH_MSG_INFO("outputHost[1000]   = " << outputHost[1000]);
      ATH_MSG_INFO("outputHost[999999] = " << outputHost[999999]);

      // Free the device memory.
      sycl::free(inputDevice, queue);
      sycl::free(outputDevice, queue);

      // Return gracefully.
      return StatusCode::SUCCESS;
   }

} // namespace GPUTutorial
